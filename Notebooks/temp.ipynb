{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e887a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from collections import defaultdict, Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3404eb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>two young guys with shaggy hair look at their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>two young , white males are outside near many ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>a man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158910</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>a man in shorts and a hawaiian shirt leans ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158911</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>a young man hanging over the side of a boat , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158912</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>a man is leaning off of the side of a blue and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158913</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>a man riding a small boat in a harbor , with f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158914</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>a man on a moored blue and white boat with hil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158875 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_name  comment_number  \\\n",
       "0       1000092795.jpg               0   \n",
       "1       1000092795.jpg               1   \n",
       "2       1000092795.jpg               2   \n",
       "3       1000092795.jpg               3   \n",
       "4       1000092795.jpg               4   \n",
       "...                ...             ...   \n",
       "158910   998845445.jpg               0   \n",
       "158911   998845445.jpg               1   \n",
       "158912   998845445.jpg               2   \n",
       "158913   998845445.jpg               3   \n",
       "158914   998845445.jpg               4   \n",
       "\n",
       "                                                  comment  \n",
       "0       two young guys with shaggy hair look at their ...  \n",
       "1       two young , white males are outside near many ...  \n",
       "2        two men in green shirts are standing in a yard .  \n",
       "3            a man in a blue shirt standing in a garden .  \n",
       "4                 two friends enjoy time spent together .  \n",
       "...                                                   ...  \n",
       "158910  a man in shorts and a hawaiian shirt leans ove...  \n",
       "158911  a young man hanging over the side of a boat , ...  \n",
       "158912  a man is leaning off of the side of a blue and...  \n",
       "158913  a man riding a small boat in a harbor , with f...  \n",
       "158914  a man on a moored blue and white boat with hil...  \n",
       "\n",
       "[158875 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#open csv file\n",
    "df = pd.read_csv(r\"..\\dataset\\flickr30k_images\\results.csv\",delimiter=\"|\")\n",
    "\n",
    "#convert to lowercase and remove whitespace str.lower() is used instead of lower() as we have a \n",
    "#pandas series here and we want to apply the operation to every string. It is an vectorized \n",
    "# operation\n",
    "df[\" comment\"] = df[\" comment\"].str.lower().str.strip()\n",
    "df = df[df[\" comment\"].notna()]                    # Remove NaN\n",
    "df = df[df[\" comment\"] != \"\"]  \n",
    "#remove duplicate same caption for same image\n",
    "df = df.drop_duplicates(subset=[\"image_name\",\" comment\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3815f838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1000092795.jpg',\n",
       " 'two young guys with shaggy hair look at their hands while hanging out in the yard .')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name = df.iloc[0][\"image_name\"]\n",
    "caption = df.iloc[0][\" comment\"]\n",
    "image_name,caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442e9175",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_dict = defaultdict(list)\n",
    "#accessing element not in default dict will not return error,\n",
    "#  it creates default value([] here)\n",
    "\n",
    "for _,row in df.iterrows():\n",
    "    caption_dict[row[\"image_name\"]].append(row[\" comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58228805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.freqs = Counter()\n",
    "        self.special_tokens = [\"<start>\",\"<end>\",\"<unk>\",\"<pad>\"]\n",
    "\n",
    "    def build_vocab(self, caption_dict, threshold=5):\n",
    "        for captions in caption_dict.values():\n",
    "            for caption in captions:\n",
    "                if not isinstance(caption, str):\n",
    "                    continue\n",
    "                words = caption.lower().strip().split()\n",
    "                self.freqs.update(words)\n",
    "\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.word2idx[token] = idx\n",
    "            self.idx2word[idx] = token\n",
    "            idx += 1\n",
    "\n",
    "        idx = len(self.special_tokens)\n",
    "\n",
    "        for word, freq in self.freqs.items():\n",
    "            if freq >= threshold:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx[\"<unk>\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    def idx_to_word(self, index):\n",
    "        return self.idx2word.get(index, \"<unk>\")\n",
    "    \n",
    "    def decode_caption(self, token_ids):\n",
    "        words = []\n",
    "\n",
    "        for token_id in token_ids:\n",
    "            #token_id = token_id.item()\n",
    "            word = self.idx_to_word(token_id)\n",
    "\n",
    "            if word == \"<end>\":\n",
    "                break\n",
    "\n",
    "            if word not in [\"<start>\", \"<pad>\"]:\n",
    "                words.append(word)\n",
    "\n",
    "        caption = \" \".join(words)\n",
    "        return caption\n",
    "    \n",
    "    def decode_captions_debugger(self, token_ids):\n",
    "        words = []\n",
    "\n",
    "        for token_id in token_ids:\n",
    "            #token_id = token_id.item()\n",
    "            word = self.idx_to_word(token_id)\n",
    "            words.append(word)\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7234e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr30kDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, caption_dict, vocab, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.caption_dict = list(caption_dict.items()) # [(image_name, [captions])]\n",
    "        self.vocab = vocab #dictionary of all words used with index allocated for each word\n",
    "        self.transform = transform #transformations to be applied for images\n",
    "\n",
    "    def __len__(self):#returns number of images present\n",
    "        return len(self.caption_dict)\n",
    "\n",
    "    def __getitem__(self, idx): #What Happens on dataset[i]\n",
    "        image_name, captions = self.caption_dict[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")#to ensure image has 3 channels\n",
    "\n",
    "        #one caption is chosen at random, but over many epoch all captions will be choosed\n",
    "        caption = random.choice(captions) \n",
    "\n",
    "        #tokenization words into numerical tensors\n",
    "        tokens = [self.vocab[\"<start>\"]]\n",
    "        tokens += [self.vocab[word] for word in caption.split()]\n",
    "        tokens += [self.vocab[\"<end>\"]]\n",
    "\n",
    "        #apply image transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fdce2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images, captions = zip(*batch)\n",
    "\n",
    "        #pad sequences \n",
    "        lengths = [len(caption) for caption in captions]\n",
    "        captions_padded = pad_sequence(captions, batch_first = True, padding_value=self.pad_idx)\n",
    "\n",
    "        return torch.stack(images), captions_padded, torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57291a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = sorted(caption_dict.items(), key=lambda x:x[0])\n",
    "seed = 42\n",
    "\n",
    "train_set, temp_set = train_test_split(items, test_size=0.25,random_state=seed)\n",
    "val_set, test_set = train_test_split(temp_set, test_size=0.5,random_state=seed)\n",
    "\n",
    "train_dict = dict(train_set)\n",
    "val_dict = dict(val_set)\n",
    "test_dict = dict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f87b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Existing Vocabulary....\n"
     ]
    }
   ],
   "source": [
    "#1. Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "#2. Vocabulary building\n",
    "\n",
    "#add pickle file\n",
    "vocab_path = r\"..\\extras\\vocabulary.pkl\"\n",
    "\n",
    "if os.path.exists(vocab_path):\n",
    "    print(\"Loading Existing Vocabulary....\")\n",
    "    with open(vocab_path, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    print(\"Creating new Vocabulary....\")\n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_vocab(caption_dict=caption_dict,\n",
    "                  threshold=4)\n",
    "    with open(vocab_path, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n",
    "\n",
    "\n",
    "#3. Collate function\n",
    "collator = Collator(pad_idx=vocab[\"<pad>\"])\n",
    "\n",
    "#4. Creating dataset\n",
    "train_dataset = Flickr30kDataset(image_dir=r\"..\\dataset\\flickr30k_images\\Images\",\n",
    "                           caption_dict=train_dict,\n",
    "                           vocab=vocab,\n",
    "                           transform=transform)\n",
    "\n",
    "val_dataset = Flickr30kDataset(image_dir=r\"..\\dataset\\flickr30k_images\\Images\",\n",
    "                           caption_dict=val_dict,\n",
    "                           vocab=vocab,\n",
    "                           transform=transform)\n",
    "\n",
    "test_dataset = Flickr30kDataset(image_dir=r\"..\\dataset\\flickr30k_images\\Images\",\n",
    "                           caption_dict=test_dict,\n",
    "                           vocab=vocab,\n",
    "                           transform=transform)\n",
    "#5. Dataloader\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                        batch_size=32,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collator)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=32,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collator)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=32,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1e71936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 224, 224])\n",
      "Captions shape: torch.Size([32, 37])\n",
      "Lengths: tensor([14, 14, 37,  9, 32, 15,  9, 11, 23, 11,  9, 24, 23, 19, 12, 17, 24, 16,\n",
      "        19,  8, 20, 14, 13, 14, 10, 19,  9, 16, 19, 19, 31, 23])\n",
      "Image: tensor([[[[ 1.8208,  2.0948,  2.1975,  ...,  0.1254,  0.1083,  0.3823],\n",
      "          [ 1.3413,  1.7009,  1.6838,  ...,  0.2796,  0.3309,  0.3823],\n",
      "          [ 1.9235,  1.9749,  1.5297,  ...,  0.3138,  0.3994,  0.3823],\n",
      "          ...,\n",
      "          [-1.4672, -0.9363, -0.6623,  ...,  0.2453,  0.2967,  0.3823],\n",
      "          [-1.0219, -0.6452, -0.4054,  ...,  0.1426,  0.1768,  0.2796],\n",
      "          [-0.5938, -0.3712, -0.2856,  ..., -0.1999, -0.1486,  0.0912]],\n",
      "\n",
      "         [[ 1.8508,  2.2010,  2.3235,  ..., -0.0574, -0.0574,  0.1877],\n",
      "          [ 1.4307,  1.7808,  1.7283,  ...,  0.1176,  0.1527,  0.2052],\n",
      "          [ 2.1310,  2.1660,  1.6408,  ...,  0.1702,  0.2577,  0.2577],\n",
      "          ...,\n",
      "          [-1.3880, -0.8803, -0.6527,  ...,  0.1877,  0.2402,  0.3102],\n",
      "          [-0.9678, -0.6176, -0.4076,  ...,  0.1001,  0.1176,  0.2052],\n",
      "          [-0.5476, -0.3725, -0.2675,  ..., -0.2325, -0.1975,  0.0126]],\n",
      "\n",
      "         [[ 2.0300,  2.3960,  2.4657,  ...,  0.0605,  0.0431,  0.3045],\n",
      "          [ 1.6117,  1.9428,  1.8208,  ...,  0.2522,  0.3045,  0.3393],\n",
      "          [ 2.3960,  2.4308,  1.8383,  ...,  0.3568,  0.4265,  0.4265],\n",
      "          ...,\n",
      "          [-1.0724, -0.5670, -0.3055,  ...,  0.4265,  0.4788,  0.5311],\n",
      "          [-0.6541, -0.2881, -0.0615,  ...,  0.3393,  0.3568,  0.4265],\n",
      "          [-0.2532, -0.0267,  0.0953,  ..., -0.0092,  0.0256,  0.2348]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0777,  2.2147,  2.2318,  ...,  0.1254,  0.0741,  0.0569],\n",
      "          [ 2.1804,  2.2147,  2.2318,  ...,  0.1254,  0.0741,  0.0569],\n",
      "          [ 2.1119,  2.2147,  2.2318,  ...,  0.0912,  0.0741,  0.0398],\n",
      "          ...,\n",
      "          [-2.0494, -2.0494, -2.0494,  ..., -2.0494, -2.0323, -2.0152],\n",
      "          [-2.0494, -2.0494, -2.0494,  ..., -2.0665, -2.0323, -2.0323],\n",
      "          [-2.0323, -2.0152, -2.0152,  ..., -2.0323, -2.0152, -2.0323]],\n",
      "\n",
      "         [[ 2.1134,  2.3936,  2.3936,  ...,  0.2227,  0.2227,  0.1702],\n",
      "          [ 2.3060,  2.3936,  2.4286,  ...,  0.2577,  0.2227,  0.1877],\n",
      "          [ 2.2710,  2.4111,  2.3936,  ...,  0.2402,  0.2227,  0.1877],\n",
      "          ...,\n",
      "          [-1.9657, -1.9657, -1.9657,  ..., -1.9657, -1.9132, -1.9132],\n",
      "          [-1.9657, -1.9657, -1.9657,  ..., -1.9657, -1.9307, -1.9132],\n",
      "          [-1.9482, -1.9307, -1.9307,  ..., -1.9482, -1.9307, -1.9482]],\n",
      "\n",
      "         [[ 2.5703,  2.5703,  2.6051,  ...,  0.3742,  0.3742,  0.3568],\n",
      "          [ 2.5703,  2.5877,  2.6226,  ...,  0.4091,  0.3742,  0.3568],\n",
      "          [ 2.5703,  2.5703,  2.6051,  ...,  0.3916,  0.3568,  0.3393],\n",
      "          ...,\n",
      "          [-1.7347, -1.7347, -1.7347,  ..., -1.7347, -1.6999, -1.6999],\n",
      "          [-1.7347, -1.7347, -1.7347,  ..., -1.7347, -1.6999, -1.6999],\n",
      "          [-1.7173, -1.6999, -1.6999,  ..., -1.7173, -1.6999, -1.7173]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9988,  0.8961,  0.8447,  ...,  2.0605,  2.0605,  2.0777],\n",
      "          [ 0.9132,  0.8276,  0.7591,  ...,  2.0777,  2.0605,  2.0777],\n",
      "          [ 0.8447,  0.8276,  0.7248,  ...,  2.0777,  2.0777,  2.0777],\n",
      "          ...,\n",
      "          [ 1.4269,  1.4269,  1.4269,  ...,  1.3242,  1.3584,  1.2557],\n",
      "          [ 1.4440,  1.4783,  1.4440,  ...,  1.3927,  1.3927,  1.4612],\n",
      "          [ 1.4098,  1.4098,  1.4098,  ...,  1.4098,  1.4440,  1.4954]],\n",
      "\n",
      "         [[ 0.5553,  0.4503,  0.3978,  ...,  2.1134,  2.1134,  2.1310],\n",
      "          [ 0.4678,  0.3803,  0.3102,  ...,  2.1310,  2.1134,  2.1310],\n",
      "          [ 0.3978,  0.3803,  0.2752,  ...,  2.1310,  2.1310,  2.1310],\n",
      "          ...,\n",
      "          [ 1.2731,  1.2731,  1.2556,  ...,  0.8704,  0.8880,  0.7829],\n",
      "          [ 1.2906,  1.3256,  1.2906,  ...,  0.9930,  0.9930,  1.0630],\n",
      "          [ 1.2556,  1.2556,  1.2556,  ...,  1.0105,  1.0455,  1.0980]],\n",
      "\n",
      "         [[ 0.6008,  0.4962,  0.4439,  ...,  2.4483,  2.4483,  2.4657],\n",
      "          [ 0.5136,  0.4265,  0.3568,  ...,  2.4657,  2.4483,  2.4657],\n",
      "          [ 0.4439,  0.4265,  0.3219,  ...,  2.4657,  2.4657,  2.4657],\n",
      "          ...,\n",
      "          [ 1.1411,  1.1759,  1.1934,  ...,  0.7402,  0.7751,  0.6705],\n",
      "          [ 1.2457,  1.2805,  1.2631,  ...,  0.8448,  0.8448,  0.9145],\n",
      "          [ 1.2282,  1.2282,  1.2282,  ...,  0.8622,  0.8971,  0.9494]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.3138,  0.3309,  0.3823,  ...,  0.4166,  0.3994,  0.3652],\n",
      "          [ 0.3652,  0.3652,  0.3652,  ...,  0.3994,  0.3994,  0.3652],\n",
      "          [ 0.3994,  0.3994,  0.3994,  ...,  0.4166,  0.3823,  0.3652],\n",
      "          ...,\n",
      "          [ 0.7762,  0.7248,  0.7933,  ...,  0.7591,  0.8276,  0.8104],\n",
      "          [ 0.7933,  0.7933,  0.7419,  ...,  0.7933,  0.7762,  0.7419],\n",
      "          [ 0.7933,  0.8104,  0.8789,  ...,  0.7591,  0.6906,  0.6221]],\n",
      "\n",
      "         [[ 0.4853,  0.5028,  0.5553,  ...,  0.5903,  0.5728,  0.5378],\n",
      "          [ 0.5203,  0.5378,  0.5378,  ...,  0.5728,  0.5728,  0.5378],\n",
      "          [ 0.5378,  0.5553,  0.5728,  ...,  0.5903,  0.5553,  0.5378],\n",
      "          ...,\n",
      "          [ 0.9230,  0.8704,  0.9405,  ...,  0.8704,  0.9405,  0.9580],\n",
      "          [ 0.9405,  0.9405,  0.8880,  ...,  0.9055,  0.8880,  0.8704],\n",
      "          [ 0.9405,  0.9580,  1.0280,  ...,  0.8880,  0.8179,  0.7304]],\n",
      "\n",
      "         [[ 0.9319,  0.9494,  0.9842,  ...,  1.0191,  1.0017,  0.9668],\n",
      "          [ 0.9494,  0.9668,  0.9668,  ...,  1.0017,  1.0017,  0.9668],\n",
      "          [ 0.9319,  0.9668,  1.0017,  ...,  1.0191,  0.9842,  0.9668],\n",
      "          ...,\n",
      "          [ 1.1759,  1.1237,  1.1585,  ...,  1.1411,  1.1759,  1.1759],\n",
      "          [ 1.1934,  1.1759,  1.1237,  ...,  1.1585,  1.1237,  1.1062],\n",
      "          [ 1.1934,  1.2108,  1.2805,  ...,  1.1062,  1.0539,  1.0017]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1804,  2.1975,  2.2147,  ...,  2.1975,  2.1975,  2.1975],\n",
      "          [ 2.1975,  2.2318,  2.1975,  ...,  2.1975,  2.1975,  2.2147],\n",
      "          [ 1.7865,  1.7865,  1.7865,  ...,  1.8550,  1.8893,  2.0263],\n",
      "          ...,\n",
      "          [-0.3198, -0.2684, -0.1657,  ..., -0.7137, -0.7650, -0.3541],\n",
      "          [-0.0972, -0.0287,  0.0398,  ..., -0.6623, -0.5938, -0.2684],\n",
      "          [-0.7479, -0.4739, -0.3883,  ..., -0.5596, -0.6281, -0.2856]],\n",
      "\n",
      "         [[ 2.3936,  2.3936,  2.3936,  ...,  2.4111,  2.3936,  2.4111],\n",
      "          [ 2.3936,  2.3936,  2.3761,  ...,  2.3936,  2.3936,  2.4111],\n",
      "          [ 2.1485,  2.1485,  2.1485,  ...,  2.2185,  2.2010,  2.2710],\n",
      "          ...,\n",
      "          [-0.4601, -0.4076, -0.2850,  ..., -0.7752, -0.8102, -0.2675],\n",
      "          [-0.2150, -0.1450, -0.0924,  ..., -0.7577, -0.6877, -0.2500],\n",
      "          [-0.7577, -0.4776, -0.4426,  ..., -0.6702, -0.7052, -0.3025]],\n",
      "\n",
      "         [[ 2.6051,  2.5877,  2.5877,  ...,  2.6051,  2.5877,  2.6051],\n",
      "          [ 2.6226,  2.6226,  2.6051,  ...,  2.6226,  2.6226,  2.6226],\n",
      "          [ 2.3437,  2.3437,  2.3437,  ...,  2.3960,  2.4308,  2.5180],\n",
      "          ...,\n",
      "          [-0.4798, -0.4624, -0.3927,  ..., -1.0724, -1.0898, -0.4973],\n",
      "          [-0.3927, -0.3404, -0.3578,  ..., -1.0724, -1.0027, -0.4798],\n",
      "          [-0.9853, -0.7238, -0.7238,  ..., -0.9678, -1.0201, -0.5321]]],\n",
      "\n",
      "\n",
      "        [[[ 2.2147,  2.2147,  2.2147,  ...,  2.2147,  2.2147,  2.2147],\n",
      "          [ 2.2147,  2.2147,  2.2147,  ...,  2.2147,  2.2147,  2.2147],\n",
      "          [ 2.2147,  2.2147,  2.2147,  ...,  2.2147,  2.2147,  2.2147],\n",
      "          ...,\n",
      "          [-1.7412, -1.7412, -1.7069,  ..., -1.2788, -1.1760, -1.2445],\n",
      "          [-1.7583, -1.8782, -1.8097,  ..., -1.3473, -1.4329, -1.3302],\n",
      "          [-1.7412, -1.7583, -1.8610,  ..., -1.3815, -1.4672, -1.5185]],\n",
      "\n",
      "         [[ 2.3936,  2.3936,  2.3936,  ...,  2.3936,  2.3936,  2.3936],\n",
      "          [ 2.3936,  2.3936,  2.3936,  ...,  2.3936,  2.3936,  2.3936],\n",
      "          [ 2.3936,  2.3936,  2.3936,  ...,  2.3936,  2.3936,  2.3936],\n",
      "          ...,\n",
      "          [-1.6155, -1.5455, -1.4405,  ..., -0.7402, -0.5651, -0.6176],\n",
      "          [-1.6155, -1.7556, -1.6506,  ..., -0.8627, -0.8978, -0.7227],\n",
      "          [-1.5630, -1.6506, -1.7381,  ..., -0.9328, -0.9853, -1.0203]],\n",
      "\n",
      "         [[ 2.6051,  2.6051,  2.6051,  ...,  2.6051,  2.6051,  2.6051],\n",
      "          [ 2.6051,  2.6051,  2.6051,  ...,  2.6051,  2.6051,  2.6051],\n",
      "          [ 2.6051,  2.6051,  2.6051,  ...,  2.6051,  2.6051,  2.6051],\n",
      "          ...,\n",
      "          [-1.5604, -1.5430, -1.4907,  ..., -1.1421, -1.1596, -1.2467],\n",
      "          [-1.5430, -1.6302, -1.5604,  ..., -1.2119, -1.2990, -1.4210],\n",
      "          [-1.5430, -1.5430, -1.6127,  ..., -1.3513, -1.4210, -1.5081]]]])\n",
      "Caption:  tensor([[   0,   33, 6546,  ...,    3,    3,    3],\n",
      "        [   0,   34,   17,  ...,    3,    3,    3],\n",
      "        [   0,   33,  994,  ..., 4631,   20,    1],\n",
      "        ...,\n",
      "        [   0,   61,  898,  ...,    3,    3,    3],\n",
      "        [   0,   33,  271,  ...,    3,    3,    3],\n",
      "        [   0,    4,  113,  ...,    3,    3,    3]])\n",
      "Sum Lengths:  tensor(553)\n"
     ]
    }
   ],
   "source": [
    "for images, captions, lengths in train_loader:\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Captions shape:\", captions.shape)    \n",
    "    print(\"Lengths:\", lengths)\n",
    "    print(\"Image:\",images)\n",
    "    print(\"Caption: \", captions)\n",
    "    print(\"Sum Lengths: \", torch.sum(lengths))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76c39975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=\"IMAGENET1K_V1\")\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False#freezing gradients, transfer learning   \n",
    "\n",
    "        modules = list(resnet.children())[:-1] #removing last layer\n",
    "        self.resnet = nn.Sequential(*modules) #combining remaining layers\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)#resizing to embedding size\n",
    "        self.bn = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)  # (B, 2048, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # (B, 2048)\n",
    "        features = self.bn(self.linear(features))  # (B, embed_size)\n",
    "        print(f\"Image features shape: {features.shape}\")\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26934ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_h = nn.Linear(embed_size, hidden_size)\n",
    "        self.init_c = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        embeddings = self.embed(captions)  # (B, T, embed_size)\n",
    "\n",
    "        # Use image features to initialize LSTM states\n",
    "        h0 = self.init_h(features).unsqueeze(0)  # (1, B, hidden_size)\n",
    "        c0 = self.init_c(features).unsqueeze(0)  # (1, B, hidden_size)\n",
    "\n",
    "        # Pack sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
    "        hiddens, _ = self.lstm(packed, (h0, c0))\n",
    "        outputs = self.linear(hiddens.data)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "    def sample(self, features, max_length = 35):\n",
    "        output_ids = []\n",
    "            # Initialize hidden and cell state using image features\n",
    "        h0 = self.init_h(features).unsqueeze(0)  # (1, B, hidden_size)\n",
    "        c0 = self.init_c(features).unsqueeze(0)  # (1, B, hidden_size)\n",
    "        states = (h0, c0)\n",
    "\n",
    "        # Start with <start> token\n",
    "        inputs = self.embed(torch.tensor([vocab[\"<start>\"]]*features.size(0)))  # (B, embed_size)\n",
    "        inputs = inputs.unsqueeze(1)  # (B, 1, embed_size)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            hidden, states = self.lstm(inputs, states)\n",
    "            outputs = self.linear(hidden.squeeze(1)) #(B,1,D) -> (B,vocab_size)\n",
    "            predicted = outputs.argmax(1) #(B,)\n",
    "            output_ids.append(predicted)\n",
    "\n",
    "            inputs = self.embed(predicted) #(B,embed_size)\n",
    "            inputs = inputs.unsqueeze(1) #(B,1,embed_size)\n",
    "\n",
    "        output_ids = torch.stack(output_ids, 1)\n",
    "        return output_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de3e06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, captions, lengths):\n",
    "        features = self.encoder(images)  # (B, embed_size)\n",
    "        outputs = self.decoder(features, captions, lengths)\n",
    "        print(f\"Model output shape: {outputs.shape}\")\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a447ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gts: ground truths, res: model results\n",
    "# Each dict should map image_id -> list of 1+ captions\n",
    "\n",
    "def evaluate_metrics(gts, res):\n",
    "    scorers = [\n",
    "        (Bleu(4), [\"BLEU-1\", \"BLEU-2\", \"BLEU-3\", \"BLEU-4\"]),\n",
    "        (Meteor(), \"METEOR\"),\n",
    "        (Cider(), \"CIDEr\"),\n",
    "        (Spice(), \"SPICE\")\n",
    "    ]\n",
    "#for bleu we get list of scores. So , we zip the scores along with names of scores.\n",
    "    results = {}\n",
    "\n",
    "    for scorer, method in scorers:\n",
    "        score, scores_per_instance = scorer.compute_score(gts, res)\n",
    "        if isinstance(method, list):\n",
    "            for m, s in zip(method, score):\n",
    "                results[m] = s #each bleu score is stored seperately in the dictionary\n",
    "        else:\n",
    "            results[method] = score #other methods and scores are stored as key-value pairs\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c8efeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image, vocab, device, max_len=20,transform = None):\n",
    "    \"\"\"\n",
    "    Generates a caption string from a single image tensor using the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if transform:\n",
    "        image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # (1, C, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encode image features\n",
    "        features = model.encoder(image)  # (1, embed_size)\n",
    "\n",
    "        # Generate token IDs using decoder\n",
    "        sampled_ids = model.decoder.sample(features, max_len=max_len)  # (1, max_len)\n",
    "        sampled_ids = sampled_ids[0].tolist()  # Remove batch dim â†’ list of ints\n",
    "\n",
    "        # Convert IDs to words\n",
    "        caption = vocab.decode_caption(sampled_ids)\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50582d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-3\n",
    "pad_idx = vocab[\"<pad>\"]\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "\n",
    "#2. Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "encoder = EncoderCNN(embed_size=embed_size)\n",
    "decoder = DecoderRNN(embed_size=embed_size, hidden_size=hidden_size, vocab_size=len(vocab))\n",
    "model = ImageCaptioningModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "\n",
    "#3. Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdb8a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, vocab, num_epochs, device, clip_value=10):\n",
    "    model.to(device=device)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        # ---------------------- TRAINING ----------------------\n",
    "        model.train() #to make model run in train mode\n",
    "        train_loss = 0 #used to track average loss per epoch        \n",
    "\n",
    "        for batch_idx, (image_batch, captions, lengths) in enumerate(train_loader):\n",
    "            image_batch = image_batch.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            adjusted_lengths = [l-1 for l in lengths]\n",
    "\n",
    "            #Targets: all words except the first one <start>\n",
    "            targets = nn.utils.rnn.pack_padded_sequence(input=captions[:,1:],\n",
    "                                                        lengths=adjusted_lengths,\n",
    "                                                        batch_first=True,\n",
    "                                                        enforce_sorted=False,\n",
    "                                                        )[0]\n",
    "            #Outputs: generated from models\n",
    "            outputs = model(image_batch, captions[:, :-1], torch.tensor(adjusted_lengths))\n",
    "\n",
    "            print(f\"Captions shape: {captions.shape}\")\n",
    "            print(f\"Lengths shape: {lengths.shape}\")\n",
    "            print(f\"Outputs shape: {outputs.shape}\")\n",
    "            print(f\"Targets shape: {targets.shape}\")\n",
    "\n",
    "            #compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            #reset gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "            #Update weights based on gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() #accumulate epoch loss\n",
    "\n",
    "            #batch loss\n",
    "            if (batch_idx ) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        #average loss in an epoch     \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "        # ---------------------- SUMMARY ----------------------\n",
    "        print(f\"\\nEpoch [{epoch}/{num_epochs}] Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc3d6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([439, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([439, 8640])\n",
      "Targets shape: torch.Size([439])\n",
      "Epoch [1/20], Step [1/745], Loss: 9.0536\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([458, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([458, 8640])\n",
      "Targets shape: torch.Size([458])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([446, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([446, 8640])\n",
      "Targets shape: torch.Size([446])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([502, 8640])\n",
      "Captions shape: torch.Size([32, 46])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([502, 8640])\n",
      "Targets shape: torch.Size([502])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([491, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([491, 8640])\n",
      "Targets shape: torch.Size([491])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([405, 8640])\n",
      "Captions shape: torch.Size([32, 22])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([405, 8640])\n",
      "Targets shape: torch.Size([405])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([444, 8640])\n",
      "Captions shape: torch.Size([32, 39])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([444, 8640])\n",
      "Targets shape: torch.Size([444])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([481, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([481, 8640])\n",
      "Targets shape: torch.Size([481])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([435, 8640])\n",
      "Captions shape: torch.Size([32, 58])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([435, 8640])\n",
      "Targets shape: torch.Size([435])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([479, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([479, 8640])\n",
      "Targets shape: torch.Size([479])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([418, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([418, 8640])\n",
      "Targets shape: torch.Size([418])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([487, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([487, 8640])\n",
      "Targets shape: torch.Size([487])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([428, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([428, 8640])\n",
      "Targets shape: torch.Size([428])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([452, 8640])\n",
      "Captions shape: torch.Size([32, 37])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([452, 8640])\n",
      "Targets shape: torch.Size([452])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([421, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([421, 8640])\n",
      "Targets shape: torch.Size([421])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([505, 8640])\n",
      "Captions shape: torch.Size([32, 36])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([505, 8640])\n",
      "Targets shape: torch.Size([505])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([558, 8640])\n",
      "Captions shape: torch.Size([32, 42])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([558, 8640])\n",
      "Targets shape: torch.Size([558])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([461, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([461, 8640])\n",
      "Targets shape: torch.Size([461])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([475, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([475, 8640])\n",
      "Targets shape: torch.Size([475])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([498, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([498, 8640])\n",
      "Targets shape: torch.Size([498])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([504, 8640])\n",
      "Captions shape: torch.Size([32, 41])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([504, 8640])\n",
      "Targets shape: torch.Size([504])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([481, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([481, 8640])\n",
      "Targets shape: torch.Size([481])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([493, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([493, 8640])\n",
      "Targets shape: torch.Size([493])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([465, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([465, 8640])\n",
      "Targets shape: torch.Size([465])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([471, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([471, 8640])\n",
      "Targets shape: torch.Size([471])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([496, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([496, 8640])\n",
      "Targets shape: torch.Size([496])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([474, 8640])\n",
      "Captions shape: torch.Size([32, 43])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([474, 8640])\n",
      "Targets shape: torch.Size([474])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([483, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([483, 8640])\n",
      "Targets shape: torch.Size([483])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([429, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([429, 8640])\n",
      "Targets shape: torch.Size([429])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([449, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([449, 8640])\n",
      "Targets shape: torch.Size([449])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([425, 8640])\n",
      "Captions shape: torch.Size([32, 22])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([425, 8640])\n",
      "Targets shape: torch.Size([425])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([444, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([444, 8640])\n",
      "Targets shape: torch.Size([444])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([449, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([449, 8640])\n",
      "Targets shape: torch.Size([449])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([479, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([479, 8640])\n",
      "Targets shape: torch.Size([479])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([459, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([459, 8640])\n",
      "Targets shape: torch.Size([459])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([428, 8640])\n",
      "Captions shape: torch.Size([32, 40])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([428, 8640])\n",
      "Targets shape: torch.Size([428])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([465, 8640])\n",
      "Captions shape: torch.Size([32, 39])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([465, 8640])\n",
      "Targets shape: torch.Size([465])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([447, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([447, 8640])\n",
      "Targets shape: torch.Size([447])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([463, 8640])\n",
      "Captions shape: torch.Size([32, 22])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([463, 8640])\n",
      "Targets shape: torch.Size([463])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([470, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([470, 8640])\n",
      "Targets shape: torch.Size([470])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([514, 8640])\n",
      "Captions shape: torch.Size([32, 50])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([514, 8640])\n",
      "Targets shape: torch.Size([514])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([463, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([463, 8640])\n",
      "Targets shape: torch.Size([463])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([412, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([412, 8640])\n",
      "Targets shape: torch.Size([412])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([413, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([413, 8640])\n",
      "Targets shape: torch.Size([413])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([494, 8640])\n",
      "Captions shape: torch.Size([32, 41])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([494, 8640])\n",
      "Targets shape: torch.Size([494])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([426, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([426, 8640])\n",
      "Targets shape: torch.Size([426])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([478, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([478, 8640])\n",
      "Targets shape: torch.Size([478])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([474, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([474, 8640])\n",
      "Targets shape: torch.Size([474])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([432, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([432, 8640])\n",
      "Targets shape: torch.Size([432])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([464, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([464, 8640])\n",
      "Targets shape: torch.Size([464])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([471, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([471, 8640])\n",
      "Targets shape: torch.Size([471])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([479, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([479, 8640])\n",
      "Targets shape: torch.Size([479])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([484, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([484, 8640])\n",
      "Targets shape: torch.Size([484])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([448, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([448, 8640])\n",
      "Targets shape: torch.Size([448])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([443, 8640])\n",
      "Captions shape: torch.Size([32, 41])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([443, 8640])\n",
      "Targets shape: torch.Size([443])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([392, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([392, 8640])\n",
      "Targets shape: torch.Size([392])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([441, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([441, 8640])\n",
      "Targets shape: torch.Size([441])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([447, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([447, 8640])\n",
      "Targets shape: torch.Size([447])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([425, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([425, 8640])\n",
      "Targets shape: torch.Size([425])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([438, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([438, 8640])\n",
      "Targets shape: torch.Size([438])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([450, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([450, 8640])\n",
      "Targets shape: torch.Size([450])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([438, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([438, 8640])\n",
      "Targets shape: torch.Size([438])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([517, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([517, 8640])\n",
      "Targets shape: torch.Size([517])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([476, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([476, 8640])\n",
      "Targets shape: torch.Size([476])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([431, 8640])\n",
      "Captions shape: torch.Size([32, 23])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([431, 8640])\n",
      "Targets shape: torch.Size([431])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([467, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([467, 8640])\n",
      "Targets shape: torch.Size([467])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([501, 8640])\n",
      "Captions shape: torch.Size([32, 50])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([501, 8640])\n",
      "Targets shape: torch.Size([501])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([486, 8640])\n",
      "Captions shape: torch.Size([32, 43])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([486, 8640])\n",
      "Targets shape: torch.Size([486])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([454, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([454, 8640])\n",
      "Targets shape: torch.Size([454])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([487, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([487, 8640])\n",
      "Targets shape: torch.Size([487])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([455, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([455, 8640])\n",
      "Targets shape: torch.Size([455])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([403, 8640])\n",
      "Captions shape: torch.Size([32, 23])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([403, 8640])\n",
      "Targets shape: torch.Size([403])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([441, 8640])\n",
      "Captions shape: torch.Size([32, 44])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([441, 8640])\n",
      "Targets shape: torch.Size([441])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([503, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([503, 8640])\n",
      "Targets shape: torch.Size([503])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([554, 8640])\n",
      "Captions shape: torch.Size([32, 41])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([554, 8640])\n",
      "Targets shape: torch.Size([554])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([403, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([403, 8640])\n",
      "Targets shape: torch.Size([403])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([411, 8640])\n",
      "Captions shape: torch.Size([32, 21])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([411, 8640])\n",
      "Targets shape: torch.Size([411])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([464, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([464, 8640])\n",
      "Targets shape: torch.Size([464])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([458, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([458, 8640])\n",
      "Targets shape: torch.Size([458])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([483, 8640])\n",
      "Captions shape: torch.Size([32, 37])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([483, 8640])\n",
      "Targets shape: torch.Size([483])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([439, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([439, 8640])\n",
      "Targets shape: torch.Size([439])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([460, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([460, 8640])\n",
      "Targets shape: torch.Size([460])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([442, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([442, 8640])\n",
      "Targets shape: torch.Size([442])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([457, 8640])\n",
      "Captions shape: torch.Size([32, 23])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([457, 8640])\n",
      "Targets shape: torch.Size([457])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([450, 8640])\n",
      "Captions shape: torch.Size([32, 37])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([450, 8640])\n",
      "Targets shape: torch.Size([450])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([423, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([423, 8640])\n",
      "Targets shape: torch.Size([423])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([460, 8640])\n",
      "Captions shape: torch.Size([32, 36])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([460, 8640])\n",
      "Targets shape: torch.Size([460])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([461, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([461, 8640])\n",
      "Targets shape: torch.Size([461])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([511, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([511, 8640])\n",
      "Targets shape: torch.Size([511])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([499, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([499, 8640])\n",
      "Targets shape: torch.Size([499])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([467, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([467, 8640])\n",
      "Targets shape: torch.Size([467])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([429, 8640])\n",
      "Captions shape: torch.Size([32, 22])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([429, 8640])\n",
      "Targets shape: torch.Size([429])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([431, 8640])\n",
      "Captions shape: torch.Size([32, 23])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([431, 8640])\n",
      "Targets shape: torch.Size([431])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([469, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([469, 8640])\n",
      "Targets shape: torch.Size([469])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([511, 8640])\n",
      "Captions shape: torch.Size([32, 36])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([511, 8640])\n",
      "Targets shape: torch.Size([511])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([487, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([487, 8640])\n",
      "Targets shape: torch.Size([487])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([437, 8640])\n",
      "Captions shape: torch.Size([32, 22])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([437, 8640])\n",
      "Targets shape: torch.Size([437])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([466, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([466, 8640])\n",
      "Targets shape: torch.Size([466])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([503, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([503, 8640])\n",
      "Targets shape: torch.Size([503])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([454, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([454, 8640])\n",
      "Targets shape: torch.Size([454])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([451, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([451, 8640])\n",
      "Targets shape: torch.Size([451])\n",
      "Epoch [1/20], Step [101/745], Loss: 4.1893\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([472, 8640])\n",
      "Captions shape: torch.Size([32, 38])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([472, 8640])\n",
      "Targets shape: torch.Size([472])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([458, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([458, 8640])\n",
      "Targets shape: torch.Size([458])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([490, 8640])\n",
      "Captions shape: torch.Size([32, 42])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([490, 8640])\n",
      "Targets shape: torch.Size([490])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([480, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([480, 8640])\n",
      "Targets shape: torch.Size([480])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([426, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([426, 8640])\n",
      "Targets shape: torch.Size([426])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([427, 8640])\n",
      "Captions shape: torch.Size([32, 39])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([427, 8640])\n",
      "Targets shape: torch.Size([427])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([441, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([441, 8640])\n",
      "Targets shape: torch.Size([441])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([448, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([448, 8640])\n",
      "Targets shape: torch.Size([448])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([436, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([436, 8640])\n",
      "Targets shape: torch.Size([436])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([473, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([473, 8640])\n",
      "Targets shape: torch.Size([473])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([481, 8640])\n",
      "Captions shape: torch.Size([32, 37])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([481, 8640])\n",
      "Targets shape: torch.Size([481])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([531, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([531, 8640])\n",
      "Targets shape: torch.Size([531])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([429, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([429, 8640])\n",
      "Targets shape: torch.Size([429])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([463, 8640])\n",
      "Captions shape: torch.Size([32, 45])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([463, 8640])\n",
      "Targets shape: torch.Size([463])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([485, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([485, 8640])\n",
      "Targets shape: torch.Size([485])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([499, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([499, 8640])\n",
      "Targets shape: torch.Size([499])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([482, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([482, 8640])\n",
      "Targets shape: torch.Size([482])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([439, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([439, 8640])\n",
      "Targets shape: torch.Size([439])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([419, 8640])\n",
      "Captions shape: torch.Size([32, 38])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([419, 8640])\n",
      "Targets shape: torch.Size([419])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([427, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([427, 8640])\n",
      "Targets shape: torch.Size([427])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([480, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([480, 8640])\n",
      "Targets shape: torch.Size([480])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([472, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([472, 8640])\n",
      "Targets shape: torch.Size([472])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([476, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([476, 8640])\n",
      "Targets shape: torch.Size([476])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([449, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([449, 8640])\n",
      "Targets shape: torch.Size([449])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([514, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([514, 8640])\n",
      "Targets shape: torch.Size([514])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([486, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([486, 8640])\n",
      "Targets shape: torch.Size([486])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([467, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([467, 8640])\n",
      "Targets shape: torch.Size([467])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([441, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([441, 8640])\n",
      "Targets shape: torch.Size([441])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([417, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([417, 8640])\n",
      "Targets shape: torch.Size([417])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([445, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([445, 8640])\n",
      "Targets shape: torch.Size([445])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([448, 8640])\n",
      "Captions shape: torch.Size([32, 39])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([448, 8640])\n",
      "Targets shape: torch.Size([448])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([451, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([451, 8640])\n",
      "Targets shape: torch.Size([451])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([497, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([497, 8640])\n",
      "Targets shape: torch.Size([497])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([464, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([464, 8640])\n",
      "Targets shape: torch.Size([464])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([404, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([404, 8640])\n",
      "Targets shape: torch.Size([404])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([448, 8640])\n",
      "Captions shape: torch.Size([32, 43])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([448, 8640])\n",
      "Targets shape: torch.Size([448])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([524, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([524, 8640])\n",
      "Targets shape: torch.Size([524])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([441, 8640])\n",
      "Captions shape: torch.Size([32, 36])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([441, 8640])\n",
      "Targets shape: torch.Size([441])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([542, 8640])\n",
      "Captions shape: torch.Size([32, 46])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([542, 8640])\n",
      "Targets shape: torch.Size([542])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([490, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([490, 8640])\n",
      "Targets shape: torch.Size([490])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([478, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([478, 8640])\n",
      "Targets shape: torch.Size([478])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([486, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([486, 8640])\n",
      "Targets shape: torch.Size([486])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([408, 8640])\n",
      "Captions shape: torch.Size([32, 22])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([408, 8640])\n",
      "Targets shape: torch.Size([408])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([475, 8640])\n",
      "Captions shape: torch.Size([32, 38])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([475, 8640])\n",
      "Targets shape: torch.Size([475])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([527, 8640])\n",
      "Captions shape: torch.Size([32, 45])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([527, 8640])\n",
      "Targets shape: torch.Size([527])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([503, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([503, 8640])\n",
      "Targets shape: torch.Size([503])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([461, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([461, 8640])\n",
      "Targets shape: torch.Size([461])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([510, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([510, 8640])\n",
      "Targets shape: torch.Size([510])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([426, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([426, 8640])\n",
      "Targets shape: torch.Size([426])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([451, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([451, 8640])\n",
      "Targets shape: torch.Size([451])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([455, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([455, 8640])\n",
      "Targets shape: torch.Size([455])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([443, 8640])\n",
      "Captions shape: torch.Size([32, 39])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([443, 8640])\n",
      "Targets shape: torch.Size([443])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([477, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([477, 8640])\n",
      "Targets shape: torch.Size([477])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([412, 8640])\n",
      "Captions shape: torch.Size([32, 21])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([412, 8640])\n",
      "Targets shape: torch.Size([412])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([464, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([464, 8640])\n",
      "Targets shape: torch.Size([464])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([422, 8640])\n",
      "Captions shape: torch.Size([32, 22])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([422, 8640])\n",
      "Targets shape: torch.Size([422])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([458, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([458, 8640])\n",
      "Targets shape: torch.Size([458])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([454, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([454, 8640])\n",
      "Targets shape: torch.Size([454])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([460, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([460, 8640])\n",
      "Targets shape: torch.Size([460])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([420, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([420, 8640])\n",
      "Targets shape: torch.Size([420])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([459, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([459, 8640])\n",
      "Targets shape: torch.Size([459])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([412, 8640])\n",
      "Captions shape: torch.Size([32, 20])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([412, 8640])\n",
      "Targets shape: torch.Size([412])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([521, 8640])\n",
      "Captions shape: torch.Size([32, 44])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([521, 8640])\n",
      "Targets shape: torch.Size([521])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([476, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([476, 8640])\n",
      "Targets shape: torch.Size([476])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([419, 8640])\n",
      "Captions shape: torch.Size([32, 23])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([419, 8640])\n",
      "Targets shape: torch.Size([419])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([450, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([450, 8640])\n",
      "Targets shape: torch.Size([450])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([452, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([452, 8640])\n",
      "Targets shape: torch.Size([452])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([459, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([459, 8640])\n",
      "Targets shape: torch.Size([459])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([506, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([506, 8640])\n",
      "Targets shape: torch.Size([506])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([474, 8640])\n",
      "Captions shape: torch.Size([32, 42])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([474, 8640])\n",
      "Targets shape: torch.Size([474])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([415, 8640])\n",
      "Captions shape: torch.Size([32, 37])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([415, 8640])\n",
      "Targets shape: torch.Size([415])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([462, 8640])\n",
      "Captions shape: torch.Size([32, 38])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([462, 8640])\n",
      "Targets shape: torch.Size([462])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([434, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([434, 8640])\n",
      "Targets shape: torch.Size([434])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([468, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([468, 8640])\n",
      "Targets shape: torch.Size([468])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([404, 8640])\n",
      "Captions shape: torch.Size([32, 23])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([404, 8640])\n",
      "Targets shape: torch.Size([404])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([456, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([456, 8640])\n",
      "Targets shape: torch.Size([456])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([529, 8640])\n",
      "Captions shape: torch.Size([32, 49])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([529, 8640])\n",
      "Targets shape: torch.Size([529])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([457, 8640])\n",
      "Captions shape: torch.Size([32, 38])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([457, 8640])\n",
      "Targets shape: torch.Size([457])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([430, 8640])\n",
      "Captions shape: torch.Size([32, 36])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([430, 8640])\n",
      "Targets shape: torch.Size([430])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([444, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([444, 8640])\n",
      "Targets shape: torch.Size([444])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([455, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([455, 8640])\n",
      "Targets shape: torch.Size([455])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([460, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([460, 8640])\n",
      "Targets shape: torch.Size([460])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([465, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([465, 8640])\n",
      "Targets shape: torch.Size([465])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([482, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([482, 8640])\n",
      "Targets shape: torch.Size([482])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([507, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([507, 8640])\n",
      "Targets shape: torch.Size([507])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([460, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([460, 8640])\n",
      "Targets shape: torch.Size([460])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([510, 8640])\n",
      "Captions shape: torch.Size([32, 37])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([510, 8640])\n",
      "Targets shape: torch.Size([510])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([485, 8640])\n",
      "Captions shape: torch.Size([32, 36])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([485, 8640])\n",
      "Targets shape: torch.Size([485])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([473, 8640])\n",
      "Captions shape: torch.Size([32, 38])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([473, 8640])\n",
      "Targets shape: torch.Size([473])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([460, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([460, 8640])\n",
      "Targets shape: torch.Size([460])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([482, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([482, 8640])\n",
      "Targets shape: torch.Size([482])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([471, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([471, 8640])\n",
      "Targets shape: torch.Size([471])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([426, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([426, 8640])\n",
      "Targets shape: torch.Size([426])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([385, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([385, 8640])\n",
      "Targets shape: torch.Size([385])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([440, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([440, 8640])\n",
      "Targets shape: torch.Size([440])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([482, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([482, 8640])\n",
      "Targets shape: torch.Size([482])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([448, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([448, 8640])\n",
      "Targets shape: torch.Size([448])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([469, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([469, 8640])\n",
      "Targets shape: torch.Size([469])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([489, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([489, 8640])\n",
      "Targets shape: torch.Size([489])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([470, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([470, 8640])\n",
      "Targets shape: torch.Size([470])\n",
      "Epoch [1/20], Step [201/745], Loss: 4.3794\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([469, 8640])\n",
      "Captions shape: torch.Size([32, 37])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([469, 8640])\n",
      "Targets shape: torch.Size([469])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([482, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([482, 8640])\n",
      "Targets shape: torch.Size([482])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([450, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([450, 8640])\n",
      "Targets shape: torch.Size([450])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([477, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([477, 8640])\n",
      "Targets shape: torch.Size([477])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([391, 8640])\n",
      "Captions shape: torch.Size([32, 21])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([391, 8640])\n",
      "Targets shape: torch.Size([391])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([449, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([449, 8640])\n",
      "Targets shape: torch.Size([449])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([425, 8640])\n",
      "Captions shape: torch.Size([32, 21])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([425, 8640])\n",
      "Targets shape: torch.Size([425])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([477, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([477, 8640])\n",
      "Targets shape: torch.Size([477])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([424, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([424, 8640])\n",
      "Targets shape: torch.Size([424])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([439, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([439, 8640])\n",
      "Targets shape: torch.Size([439])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([483, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([483, 8640])\n",
      "Targets shape: torch.Size([483])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([460, 8640])\n",
      "Captions shape: torch.Size([32, 23])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([460, 8640])\n",
      "Targets shape: torch.Size([460])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([474, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([474, 8640])\n",
      "Targets shape: torch.Size([474])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([443, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([443, 8640])\n",
      "Targets shape: torch.Size([443])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([419, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([419, 8640])\n",
      "Targets shape: torch.Size([419])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([461, 8640])\n",
      "Captions shape: torch.Size([32, 37])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([461, 8640])\n",
      "Targets shape: torch.Size([461])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([455, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([455, 8640])\n",
      "Targets shape: torch.Size([455])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([464, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([464, 8640])\n",
      "Targets shape: torch.Size([464])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([468, 8640])\n",
      "Captions shape: torch.Size([32, 23])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([468, 8640])\n",
      "Targets shape: torch.Size([468])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([492, 8640])\n",
      "Captions shape: torch.Size([32, 40])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([492, 8640])\n",
      "Targets shape: torch.Size([492])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([436, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([436, 8640])\n",
      "Targets shape: torch.Size([436])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([438, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([438, 8640])\n",
      "Targets shape: torch.Size([438])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([468, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([468, 8640])\n",
      "Targets shape: torch.Size([468])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([451, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([451, 8640])\n",
      "Targets shape: torch.Size([451])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([467, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([467, 8640])\n",
      "Targets shape: torch.Size([467])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([463, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([463, 8640])\n",
      "Targets shape: torch.Size([463])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([519, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([519, 8640])\n",
      "Targets shape: torch.Size([519])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([457, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([457, 8640])\n",
      "Targets shape: torch.Size([457])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([439, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([439, 8640])\n",
      "Targets shape: torch.Size([439])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([425, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([425, 8640])\n",
      "Targets shape: torch.Size([425])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([428, 8640])\n",
      "Captions shape: torch.Size([32, 37])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([428, 8640])\n",
      "Targets shape: torch.Size([428])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([443, 8640])\n",
      "Captions shape: torch.Size([32, 23])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([443, 8640])\n",
      "Targets shape: torch.Size([443])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([473, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([473, 8640])\n",
      "Targets shape: torch.Size([473])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([463, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([463, 8640])\n",
      "Targets shape: torch.Size([463])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([488, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([488, 8640])\n",
      "Targets shape: torch.Size([488])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([474, 8640])\n",
      "Captions shape: torch.Size([32, 33])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([474, 8640])\n",
      "Targets shape: torch.Size([474])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([466, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([466, 8640])\n",
      "Targets shape: torch.Size([466])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([512, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([512, 8640])\n",
      "Targets shape: torch.Size([512])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([467, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([467, 8640])\n",
      "Targets shape: torch.Size([467])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([460, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([460, 8640])\n",
      "Targets shape: torch.Size([460])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([453, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([453, 8640])\n",
      "Targets shape: torch.Size([453])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([462, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([462, 8640])\n",
      "Targets shape: torch.Size([462])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([520, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([520, 8640])\n",
      "Targets shape: torch.Size([520])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([387, 8640])\n",
      "Captions shape: torch.Size([32, 20])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([387, 8640])\n",
      "Targets shape: torch.Size([387])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([448, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([448, 8640])\n",
      "Targets shape: torch.Size([448])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([487, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([487, 8640])\n",
      "Targets shape: torch.Size([487])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([466, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([466, 8640])\n",
      "Targets shape: torch.Size([466])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([433, 8640])\n",
      "Captions shape: torch.Size([32, 34])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([433, 8640])\n",
      "Targets shape: torch.Size([433])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([478, 8640])\n",
      "Captions shape: torch.Size([32, 28])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([478, 8640])\n",
      "Targets shape: torch.Size([478])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([463, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([463, 8640])\n",
      "Targets shape: torch.Size([463])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([489, 8640])\n",
      "Captions shape: torch.Size([32, 40])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([489, 8640])\n",
      "Targets shape: torch.Size([489])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([508, 8640])\n",
      "Captions shape: torch.Size([32, 43])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([508, 8640])\n",
      "Targets shape: torch.Size([508])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([425, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([425, 8640])\n",
      "Targets shape: torch.Size([425])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([425, 8640])\n",
      "Captions shape: torch.Size([32, 24])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([425, 8640])\n",
      "Targets shape: torch.Size([425])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([431, 8640])\n",
      "Captions shape: torch.Size([32, 20])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([431, 8640])\n",
      "Targets shape: torch.Size([431])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([434, 8640])\n",
      "Captions shape: torch.Size([32, 30])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([434, 8640])\n",
      "Targets shape: torch.Size([434])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([477, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([477, 8640])\n",
      "Targets shape: torch.Size([477])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([443, 8640])\n",
      "Captions shape: torch.Size([32, 37])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([443, 8640])\n",
      "Targets shape: torch.Size([443])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([423, 8640])\n",
      "Captions shape: torch.Size([32, 36])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([423, 8640])\n",
      "Targets shape: torch.Size([423])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([512, 8640])\n",
      "Captions shape: torch.Size([32, 45])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([512, 8640])\n",
      "Targets shape: torch.Size([512])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([465, 8640])\n",
      "Captions shape: torch.Size([32, 23])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([465, 8640])\n",
      "Targets shape: torch.Size([465])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([449, 8640])\n",
      "Captions shape: torch.Size([32, 31])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([449, 8640])\n",
      "Targets shape: torch.Size([449])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([460, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([460, 8640])\n",
      "Targets shape: torch.Size([460])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([439, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([439, 8640])\n",
      "Targets shape: torch.Size([439])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([440, 8640])\n",
      "Captions shape: torch.Size([32, 35])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([440, 8640])\n",
      "Targets shape: torch.Size([440])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([476, 8640])\n",
      "Captions shape: torch.Size([32, 27])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([476, 8640])\n",
      "Targets shape: torch.Size([476])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([420, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([420, 8640])\n",
      "Targets shape: torch.Size([420])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([461, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([461, 8640])\n",
      "Targets shape: torch.Size([461])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([396, 8640])\n",
      "Captions shape: torch.Size([32, 32])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([396, 8640])\n",
      "Targets shape: torch.Size([396])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([462, 8640])\n",
      "Captions shape: torch.Size([32, 29])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([462, 8640])\n",
      "Targets shape: torch.Size([462])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([515, 8640])\n",
      "Captions shape: torch.Size([32, 40])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([515, 8640])\n",
      "Targets shape: torch.Size([515])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([462, 8640])\n",
      "Captions shape: torch.Size([32, 26])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([462, 8640])\n",
      "Targets shape: torch.Size([462])\n",
      "Image features shape: torch.Size([32, 256])\n",
      "Model output shape: torch.Size([497, 8640])\n",
      "Captions shape: torch.Size([32, 25])\n",
      "Lengths shape: torch.Size([32])\n",
      "Outputs shape: torch.Size([497, 8640])\n",
      "Targets shape: torch.Size([497])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      2\u001b[0m             train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m      3\u001b[0m             criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m      4\u001b[0m             optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m      5\u001b[0m             vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m      6\u001b[0m             num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[0;32m      7\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m      8\u001b[0m             clip_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, vocab, num_epochs, device, clip_value)\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m#to make model run in train mode\u001b[39;00m\n\u001b[0;32m      8\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m#used to track average loss per epoch        \u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (image_batch, captions, lengths) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     11\u001b[0m     image_batch \u001b[38;5;241m=\u001b[39m image_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m     captions \u001b[38;5;241m=\u001b[39m captions\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mFlickr30kDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     13\u001b[0m image_name, captions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaption_dict[idx]\n\u001b[0;32m     14\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, image_name)\n\u001b[1;32m---> 15\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#to ensure image has 3 channels\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#one caption is chosen at random, but over many epoch all captions will be choosed\u001b[39;00m\n\u001b[0;32m     18\u001b[0m caption \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(captions) \n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3274\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3272\u001b[0m filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[1;32m-> 3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m   3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<frozen ntpath>:714\u001b[0m, in \u001b[0;36mrealpath\u001b[1;34m(path, strict)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model=model,\n",
    "            train_loader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            vocab=vocab,\n",
    "            num_epochs=num_epochs,\n",
    "            device=device,\n",
    "            clip_value=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
